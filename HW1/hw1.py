# -*- coding: utf-8 -*-
"""HW1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12WnV2M72yf9DmBi-YqohYRXe4vnlx_2q
"""

from google.colab import drive
drive.mount('/content/drive')

x = []
y = []
path = '/content/drive/MyDrive/Machine_Learning/HW1/testfile.txt'
with open(path) as f:
    for line in f.readlines():
      s = line.split(',')
      x.append(float(s[0]))
      y.append(float(s[1]))
print(x)
print(y)

import numpy as np

def cal(A,x):
  number=len(A)-1
  ans=0
  for i in range(len(A)):
    ans+=A[i][0]*(x**number)
    number-=1
  return ans

def loss(y, y_pred):
  y=np.array(y)
  #print(len(y))
  y_pred=np.array(y_pred)
  rms = np.sqrt(np.mean((y-y_pred)**2))
  loss = np.sum((y-y_pred)**2)
  return loss
def objective(x, a, b):
	return a * x + b

def mupltipy_matrix(A,B):
  A_row=len(A)
  A_column=len(A[0])
  B_row=len(B)
  B_column=len(B[0])
  # print(A_row)
  # print(A_column)
  # print(B_column)
  if A_column!=B_row:
    return print('cannot multiplication')
  C = np.zeros((A_row,B_column))
  #print(C)
  for i in range(A_row):
    for j in range(A_column):
      for k in range(B_column):
        C[i][k]+=A[i][j]*B[j][k]
  return C

def fitting_line(A):
  a=''
  number=len(A)-1
  for i in range(len(A)):
    if number==0:
      a+=str(A[i][0])
      return a
    a+=str(A[i][0])
    a+='X^'
    a+=str(number)
    a+=' + '
    number-=1 

#using Gauss-Jordan to find a inverse_matrix
def inverse(A):
  augmented_matrix = np.zeros((len(A),2*len(A)))
  #print(augmented_matrix)
  for i in range(len(A)):
    for j in range(len(A)):
      if i==j:
        augmented_matrix[i][j+len(A)] = 1
  #print(augmented_matrix)
  for i in range(len(A)):
    for j in range(len(A)):
      augmented_matrix[i][j] = A[i][j]
  #print(augmented_matrix)
  #print(augmented_matrix)
  for i in range(len(A)):
    temp = augmented_matrix[i][i]
    for j in range(2*len(A)):
      augmented_matrix[i][j]/=temp 
    # print(temp) 
    # print(augmented_matrix)
    for k in range(len(A)):
      if i!=k:
        ratio=augmented_matrix[k][i]
        for m in range(2*len(A)):
          augmented_matrix[k][m]=augmented_matrix[k][m]-ratio*augmented_matrix[i][m]
          #print(augmented_matrix)
  #print(augmented_matrix)
  inverse_matrix=np.zeros((len(A),len(A)))
  #augmented_matrix=inverse(A)
  for i in range(len(A)):
    for j in range(len(A)):
      inverse_matrix[i][j] = augmented_matrix[i][j+len(A)]
  return inverse_matrix
def transpose(A):
  a_transpose=[]
  for i in range(len(A[0])):
    buffer=[]
    for j in range(len(A)):
      buffer.append(A[j][i])
    a_transpose.append(buffer)
  return np.array(a_transpose)

def LSE_Case(n,λ):
  number=[]
  A_T=[]
  # n=2
  # λ=0
  highest_power = n - 1
  #due to the constant, need to have all 1 in the last column of matrix A
  for i in range(n):
    buffer=[]
    if highest_power == 0:
      for j in range(len(x)):
        number.append(1)
    elif highest_power > 0:
      for j in range(len(x)):
        temp = x[j]**highest_power
        buffer.append(temp)
      A_T.append(buffer)
    highest_power -= 1
  A_T.append(number)
  A_T=np.array(A_T)
  #print(transpose(A_T))
  # for i in range(len(x)):
  #   number.append(1)
  #A_T is the transpose of A
  # A_T.append(x)

  #in case 1, A_T => 2*23 matrix
  # print(len(A_T))
  ATA=[[0]*len(A_T) for i in range(len(A_T))]
  # print(ATA)
  for i in range(len(A_T)):
    for j in range(len(A_T)):
      for k in range(len(A_T[i])):
        temp = A_T[i][k] * A_T[j][k]
        ATA[i][j] += temp
  #ATA_Matrix=np.array(ATA)
  #print(ATA)
  if λ > 0:
    λ_Matrix=np.eye(n, dtype = 'float')
    λ_Matrix*=λ
    #print(λ_Matrix)
  elif λ == 0:
    λ_Matrix=np.zeros((len(A_T),len(A_T)))
  #A --> ATA + λI
  ATA=np.array(ATA)
  A=ATA+λ_Matrix

  
  
  #inverse_matrix=np.zeros((len(A),len(A)))
  inverse_matrix=inverse(A)
  # for i in range(len(A)):
  #   for j in range(len(A)):
  #     inverse_matrix[i][j] = augmented_matrix[i][j+len(A)]
  # B_inv = np.linalg.inv(A)
  # print(B_inv)
  #print(inverse_matrix)
  #print(A_T)

  # print(transpose_y)
  # A_T=np.array(A_T)
  # print(type(A_T))
  # ans=inverse_matrix.dot(A_T.dot(transpose_y))
  # print(ans)
  y_2=[]
  y_2.append(y)
  transpose_y=np.transpose(y_2)
  # print(transpose_y)

  # x=(ATA)^(-1)*(AT)*b
  ans_x=mupltipy_matrix(inverse_matrix,mupltipy_matrix(A_T,transpose_y))
  #print(ans_x[1][0])
  return ans_x

def Newton(n,λ):
  number=[]
  A_T=[]
  X=[]
  # n=2
  # λ=0
  highest_power = n - 1
  #due to the constant, need to have all 1 in the last column of matrix A
  for i in range(n):
    buffer=[]
    if highest_power == 0:
      for j in range(len(x)):
        number.append(1)
    elif highest_power > 0:
      for j in range(len(x)):
        temp = x[j]**highest_power
        buffer.append(temp)
      A_T.append(buffer)
    highest_power -= 1
  A_T.append(number)
  A_T=np.array(A_T)
  #print(A_T)
  A=transpose(A_T)
  #print(A)
  for i in range(n):
    buffer=[0]
    X.append(buffer)
  X0=np.array(X)
  b=[]
  b.append(y)
  b=np.array(b)#1*23
  b=transpose(b)#23*1
 
  for i in range(5): # 給他收斂多次點，大多2、3次就收斂了
    Hessian_Matrix=2*mupltipy_matrix(A_T,A)
    #print(inverse(Hessian_Matrix))
    gradient=2*mupltipy_matrix((mupltipy_matrix(A_T,A)),X0)-2*mupltipy_matrix(A_T,b)
    #print(gradient)
    #inverse_Hessian_Matrix=inverse(Hessian_Matrix)
    X1=X0-mupltipy_matrix(inverse(Hessian_Matrix),gradient)
    #print(inverse_Hessian_Matrix.shape)
    #X1=X0-mupltipy_matrix(inverse_Hessian_Matrix,gradient)
    X0=X1
  return X0

#Case 1: n = 2,λ = 0 
LSE_Case1=LSE_Case(2,0)
import matplotlib.pyplot as plt
import matplotlib as mpl
import seaborn as sns
from matplotlib import pyplot
from scipy.optimize import curve_fit
from numpy import arange
plt.scatter(x,y)
# summarize the parameter values
a, b = LSE_Case1[0][0],LSE_Case1[1][0]
x_line = arange(min(x), max(x), 0.1)
# calculate the output for the range
y_line = a*x_line+b #a * x_line + b
# create a line plot for the mapping function
pyplot.plot(x_line, y_line, '-', color='black')
print('Fitting line:',fitting_line(LSE_Case1))
from sklearn.metrics import mean_squared_error


y_line=y_line.tolist()
y_pred=[]
for i in range(len(x)):
   y_pred.append(cal(LSE_Case1,x[i]))
#mse = loss(y, y_pred)
print('Total error: ',loss(y, y_pred))

#Case 1: n = 2,λ = 0 
Newton_Case1=Newton(2,0)
import matplotlib.pyplot as plt
import matplotlib as mpl
import seaborn as sns
from matplotlib import pyplot
from scipy.optimize import curve_fit
from numpy import arange
plt.scatter(x,y)
# summarize the parameter values
a, b = Newton_Case1[0][0],Newton_Case1[1][0]
x_line = arange(min(x), max(x), 0.1)
# calculate the output for the range
y_line = a*x_line+b #a * x_line + b
# create a line plot for the mapping function
pyplot.plot(x_line, y_line, '-', color='black')
print('Fitting line:',fitting_line(Newton_Case1))
from sklearn.metrics import mean_squared_error


y_line=y_line.tolist()
y_pred=[]
for i in range(len(x)):
   y_pred.append(cal(Newton_Case1,x[i]))
#mse = loss(y, y_pred)
print('Total error: ',loss(y, y_pred))

#Case 2: n = 3,λ = 0 
LSE_Case2 = LSE_Case(3,0)
#print(Case2)
import matplotlib.pyplot as plt
import matplotlib as mpl
import seaborn as sns
from matplotlib import pyplot
from scipy.optimize import curve_fit
from numpy import arange
plt.scatter(x,y)
# summarize the parameter values
a, b, c = LSE_Case2[0][0],LSE_Case2[1][0],LSE_Case2[2][0]
x_line = arange(min(x), max(x), 0.1)
# calculate the output for the range
y_line = a * (x_line**2) + b * x_line + c #ax^2+bx+c
# create a line plot for the mapping function
pyplot.plot(x_line, y_line, '-', color='black')
print('Fitting line:',fitting_line(LSE_Case2))
from sklearn.metrics import mean_squared_error


y_line=y_line.tolist()
y_pred=[]
for i in range(len(x)):
   y_pred.append(cal(LSE_Case2,x[i]))
#mse = loss(y, y_pred)
print('Total error: ',loss(y, y_pred))

#Case 2: n = 3,λ = 0 
Newton_Case2 = Newton(3,0)
#print(Case2)
import matplotlib.pyplot as plt
import matplotlib as mpl
import seaborn as sns
from matplotlib import pyplot
from scipy.optimize import curve_fit
from numpy import arange
plt.scatter(x,y)
# summarize the parameter values
a, b, c = Newton_Case2[0][0],Newton_Case2[1][0],Newton_Case2[2][0]
x_line = arange(min(x), max(x), 0.1)
# calculate the output for the range
y_line = a * (x_line**2) + b * x_line + c #ax^2+bx+c
# create a line plot for the mapping function
pyplot.plot(x_line, y_line, '-', color='black')
print('Fitting line:',fitting_line(Newton_Case2))
from sklearn.metrics import mean_squared_error


y_line=y_line.tolist()
y_pred=[]
for i in range(len(x)):
   y_pred.append(cal(Newton_Case2,x[i]))
#mse = loss(y, y_pred)
print('Total error: ',loss(y, y_pred))

#Case 3: n = 3,λ = 10000 
LSE_Case3 = LSE_Case(3,10000)
#print(Case3)
import matplotlib.pyplot as plt
import matplotlib as mpl
import seaborn as sns
from matplotlib import pyplot
from scipy.optimize import curve_fit
from numpy import arange
plt.scatter(x,y)
# summarize the parameter values
a, b, c = LSE_Case3[0][0],LSE_Case3[1][0],LSE_Case3[2][0]
x_line = arange(min(x), max(x), 0.1)
# calculate the output for the range
y_line = a * (x_line**2) + b * x_line + c #ax^2+bx+c
# create a line plot for the mapping function
pyplot.plot(x_line, y_line, '-', color='black')
print('Fitting line:',fitting_line(LSE_Case3))
from sklearn.metrics import mean_squared_error


y_line=y_line.tolist()
y_pred=[]
for i in range(len(x)):
   y_pred.append(cal(LSE_Case3,x[i]))
#mse = loss(y, y_pred)
print('Total error: ',loss(y, y_pred))

#Case 3: n = 3,λ = 10000 
Newton_Case3 = Newton(3,10000)
#print(Case3)
import matplotlib.pyplot as plt
import matplotlib as mpl
import seaborn as sns
from matplotlib import pyplot
from scipy.optimize import curve_fit
from numpy import arange
plt.scatter(x,y)
# summarize the parameter values
a, b, c = Newton_Case3[0][0],Newton_Case3[1][0],Newton_Case3[2][0]
x_line = arange(min(x), max(x), 0.1)
# calculate the output for the range
y_line = a * (x_line**2) + b * x_line + c #ax^2+bx+c
# create a line plot for the mapping function
pyplot.plot(x_line, y_line, '-', color='black')
print('Fitting line:',fitting_line(Newton_Case3))
from sklearn.metrics import mean_squared_error


y_line=y_line.tolist()
y_pred=[]
for i in range(len(x)):
   y_pred.append(cal(Newton_Case3,x[i]))
#mse = loss(y, y_pred)
print('Total error: ',loss(y, y_pred))